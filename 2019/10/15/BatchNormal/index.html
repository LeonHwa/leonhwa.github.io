<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="BatchNormalBN应用于激活函数之前，可以降低层与层之间一因为底层（左边）参数的微弱变化传到上层（右边）被放大（蝴蝶效应），上层需要不断地去适应这些变化，使得训练变得困难(Internal Convariate Shift)。应用BN后 会使得同批次的训练集的同个特征分布均值为0，方差为1。优点如下：  使得输入数据分布相对稳定，加快学习速度 减小模型对参数敏感性，简化调参，提高训练稳定性">
<meta name="keywords" content="DL">
<meta property="og:type" content="article">
<meta property="og:title" content="BatchNormal">
<meta property="og:url" content="http://d2rivendell.github.io/2019/10/15/BatchNormal/index.html">
<meta property="og:site_name" content="rivendell">
<meta property="og:description" content="BatchNormalBN应用于激活函数之前，可以降低层与层之间一因为底层（左边）参数的微弱变化传到上层（右边）被放大（蝴蝶效应），上层需要不断地去适应这些变化，使得训练变得困难(Internal Convariate Shift)。应用BN后 会使得同批次的训练集的同个特征分布均值为0，方差为1。优点如下：  使得输入数据分布相对稳定，加快学习速度 减小模型对参数敏感性，简化调参，提高训练稳定性">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://d2rivendell.github.io/2019/10/15/BatchNormal/images/BatchNormal.png">
<meta property="og:image" content="http://d2rivendell.github.io/2019/10/15/BatchNormal/images/table3.png">
<meta property="og:updated_time" content="2020-10-23T09:07:38.911Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BatchNormal">
<meta name="twitter:description" content="BatchNormalBN应用于激活函数之前，可以降低层与层之间一因为底层（左边）参数的微弱变化传到上层（右边）被放大（蝴蝶效应），上层需要不断地去适应这些变化，使得训练变得困难(Internal Convariate Shift)。应用BN后 会使得同批次的训练集的同个特征分布均值为0，方差为1。优点如下：  使得输入数据分布相对稳定，加快学习速度 减小模型对参数敏感性，简化调参，提高训练稳定性">
<meta name="twitter:image" content="http://d2rivendell.github.io/2019/10/15/BatchNormal/images/BatchNormal.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>BatchNormal</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
    
</head>

<body class="max-width mx-auto px3 ltl">    
      <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about">About</a></li>
         
          <li><a href="/archives">Writing</a></li>
         
          <li><a href="/tags">tags</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/10/25/反向传播公式总结/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/10/15/CNN/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://d2rivendell.github.io/2019/10/15/BatchNormal/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&text=BatchNormal"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&title=BatchNormal"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&is_video=false&description=BatchNormal"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=BatchNormal&body=Check out this article: http://d2rivendell.github.io/2019/10/15/BatchNormal/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&title=BatchNormal"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&title=BatchNormal"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&title=BatchNormal"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&title=BatchNormal"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&name=BatchNormal&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#BatchNormal"><span class="toc-number">1.</span> <span class="toc-text">BatchNormal</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#求导："><span class="toc-number">1.0.1.</span> <span class="toc-text">求导：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#复合函数的链式法则"><span class="toc-number">1.0.2.</span> <span class="toc-text">复合函数的链式法则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#复合函数的偏导-链式法则"><span class="toc-number">1.0.3.</span> <span class="toc-text">复合函数的偏导 链式法则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#反向传播"><span class="toc-number">1.0.4.</span> <span class="toc-text">反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#总结"><span class="toc-number">1.0.5.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index my4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        BatchNormal
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">rivendell</span>
      </span>
      
    <div class="postdate">
        <time datetime="2019-10-15T15:46:01.000Z" itemprop="datePublished">2019-10-15</time>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/DL/">DL</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h2 id="BatchNormal"><a href="#BatchNormal" class="headerlink" title="BatchNormal"></a>BatchNormal</h2><p>BN应用于激活函数之前，可以降低层与层之间一因为底层（左边）参数的微弱变化传到上层（右边）被放大（蝴蝶效应），上层需要不断地去适应这些变化，使得训练变得困难(Internal Convariate Shift)。应用BN后 会使得同批次的训练集的同个特征分布均值为0，方差为1。优点如下：</p>
<ol>
<li>使得输入数据分布相对稳定，加快学习速度</li>
<li>减小模型对参数敏感性，简化调参，提高训练稳定性</li>
<li>缓解梯度消失问题起到一定正则效果（可以不使用dropout）</li>
</ol>
<p>公式如下：</p>
<p><img src="./images/BatchNormal.png" alt="BatchNormal" style="zoom:25%;"></p>
<p>$\epsilon$是为了防止分母出现0。</p>
<h4 id="求导："><a href="#求导：" class="headerlink" title="求导："></a>求导：</h4><script type="math/tex; mode=display">\begin{array}{l}{\frac{\partial \ell}{\partial \widehat{x}_{i}}=\frac{\partial \ell}{\partial y_{i}} \cdot \gamma}      \quad (p1).     \\ {\frac{\partial \ell}{\partial \sigma_{\mathcal{B}}^{2}}=\sum_{i=1}^{m} \frac{\partial \ell}{\partial \widehat{x}_{i}} \cdot\left(x_{i}-\mu_{\mathcal{B}}\right) \cdot \frac{-1}{2}\left(\sigma_{\mathcal{B}}^{2}+\epsilon\right)^{-3 / 2}} \quad (p2).   \\ {\frac{\partial \ell}{\partial \mu_{\mathcal{B}}}=\left(\sum_{i=1}^{m} \frac{\partial \ell}{\partial \widehat{x}_{i}} \cdot \frac{-1}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}}\right)+\frac{\partial \ell}{\partial \sigma_{\mathcal{B}}^{2}} \cdot \frac{\sum_{i=1}^{m}-2\left(x_{i}-\mu_{\mathcal{B}}\right)}{m}}     \quad (p3).   \\ {\frac{\partial \ell}{\partial x_{i}}=\frac{\partial \ell}{\partial \widehat{x}_{i}} \cdot \frac{1}{\sqrt{\sigma_{\mathcal{B}}^{2}+\epsilon}}+\frac{\partial \ell}{\partial \sigma_{\mathcal{B}}^{2}} \cdot \frac{2\left(x_{i}-\mu_{\mathcal{B}}\right)}{m}+\frac{\partial \ell}{\partial \mu_{\mathcal{B}}} \cdot \frac{1}{m}} \quad (p4). \\ {\frac{\partial \ell}{\partial \gamma}=\sum_{i=1}^{m} \frac{\partial \ell}{\partial y_{i}} \cdot \widehat{x}_{i}} \quad (p5). \\ {\frac{\partial \ell}{\partial \beta}=\sum_{i=1}^{m} \frac{\partial \ell}{\partial y_{i}}}  \quad (p6)        \end{array}</script><p>一开始很疑惑公式$p3$到底是怎么来的，不符合复合函数链式法则。复习了高数，查找了相关的资料，发现<strong>复合函数链式法则</strong>和<strong>复合函数的偏导链式法则</strong>   概念是有区别的</p>
<h4 id="复合函数的链式法则"><a href="#复合函数的链式法则" class="headerlink" title="复合函数的链式法则"></a>复合函数的链式法则</h4><p> 导数运算法则</p>
<p>  $(1) (u \pm v)^{‘}(x) = u^{‘}(x) + v{‘}(x)$</p>
<p>  $(2) (u v)^{‘}(x) = u^{‘}(x)v(x) + u(x)v{‘}(x)$</p>
<p>  $(2) \frac{u}{v}^{‘}(x) = \frac{ u^{‘}(x)v(x) - u(x)v{‘}(x)}{v^2(x)}$</p>
<p>  设有函数如下：</p>
<p>   $u = g(x), y = f[g(x)]$</p>
<p> $\frac{\partial y}{ \partial x} = f’(u) \cdot  g’(x)$或者$\frac{\partial y}{ \partial x} = \frac{\partial y}{ \partial u} \cdot  \frac{\partial u}{ \partial x}$</p>
<h4 id="复合函数的偏导-链式法则"><a href="#复合函数的偏导-链式法则" class="headerlink" title="复合函数的偏导 链式法则"></a>复合函数的偏导 链式法则</h4><p>若已知二元函数$ z=f(u,v)$，$z$ 是 $u$， $v$ 的函数，但若 $u$ 和 $v$ 都又是 $x$ 和 $y$ 的函数，则 $z$ 最终是 $x$ 和 $y$ 的函数，即</p>
<script type="math/tex; mode=display">z(x, y) = f[u(x,y), v(x,y)]</script><p>那如何求 $z$ 对 $x$ 和 $y$ 的偏微分呢？我们先来看全微分关系．首先:</p>
<script type="math/tex; mode=display">\mathrm{d} z=\frac{\partial f}{\partial u} \mathrm{d} u+\frac{\partial f}{\partial v} \mathrm{d}v</script><p>而 $u$ 和 $v$ 的微小变化又都是由 $x$ 和 $y$ 的微小变化引起的</p>
<script type="math/tex; mode=display">\mathrm{d} u=\frac{\partial u}{\partial x} \mathrm{d} x+\frac{\partial u}{\partial y} \mathrm{d} y \quad \mathrm{d} v=\frac{\partial v}{\partial x} \mathrm{d} x+\frac{\partial v}{\partial y} \mathrm{d} y</script><p>所以得出</p>
<script type="math/tex; mode=display">\begin{aligned} \mathrm{d} z &=\frac{\partial f}{\partial u}\left(\frac{\partial u}{\partial x} \mathrm{d} x+\frac{\partial u}{\partial y} \mathrm{d} y\right)+\frac{\partial f}{\partial v}\left(\frac{\partial v}{\partial x} \mathrm{d} x+\frac{\partial v}{\partial y} \mathrm{d} y\right) \\ &=\left(\frac{\partial f}{\partial u} \frac{\partial u}{\partial x}+\frac{\partial f}{\partial v} \frac{\partial v}{\partial x}\right) \mathrm{d} x+\left(\frac{\partial f}{\partial u} \frac{\partial u}{\partial y}+\frac{\partial f}{\partial v} \frac{\partial v}{\partial y}\right) \mathrm{d} y \end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} \frac{\partial z}{\partial x} = (\frac{\partial f}{\partial u} \frac{\partial u}{\partial x} + \frac{\partial f}{\partial v} \frac{\partial v}{\partial x})  \\ \frac{\partial z}{\partial y} = (\frac{\partial f}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial f}{\partial v} \frac{\partial v}{\partial y})  \end{aligned}</script><p>这就是 $z$ 关于 $x$ 和 $y$ 的全微分关系．根据定义</p>
<script type="math/tex; mode=display">\begin{array}{l}{\frac{\partial f}{\partial x}=\frac{\partial f}{\partial u} \frac{\partial u}{\partial x}+\frac{\partial f}{\partial v} \frac{\partial v}{\partial x}} \\ {\frac{\partial f}{\partial y}=\frac{\partial f}{\partial u} \frac{\partial u}{\partial y}+\frac{\partial f}{\partial v} \frac{\partial v}{\partial y}}\end{array}</script><p>也叫偏导的<strong>链式法则</strong>．</p>
<p>根据上边公式证明$p3$</p>
<p><img src="./images/table3.png" alt="table3" style="zoom:50%;"></p>
<p>$f$函数为上一层的函数,$\frac{\partial \ell}{\partial y}$为上一层传下来的导数</p>
<p>其中:</p>
<script type="math/tex; mode=display">\mu \sim   \frac{-\mu_{\beta}}{\sqrt{\sigma^{2}+\epsilon}}</script><script type="math/tex; mode=display">\sigma^2 \sim \sigma^2</script><script type="math/tex; mode=display">\begin{aligned} \frac{\partial \ell}{ \partial \mu} &= \frac{\partial \ell}{ \partial \widehat{x}} (\frac{\partial \widehat{x}}{ \partial \mu} + \frac{\partial \widehat{x}}{ \partial \sigma^{2}})  \\&= \frac{\partial \ell}{ \partial \widehat{x}} (\frac{\partial \widehat{x}}{ \partial \mu} + \frac{\partial \widehat{x}}{ \partial \sigma^{2}} \frac{\partial \sigma^{2}}{ \partial \mu} ) \\ &= \frac{\partial \ell}{ \partial \widehat{x}} \frac{\partial \widehat{x}}{\partial \mu } + \frac{\partial \ell}{ \partial \sigma^{2}} \frac{\partial \sigma^{2}}{\partial \mu }\end{aligned}</script><p>   证毕</p>
<h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>反向传播要求出输入$x$和$\gamma ,\beta$的导数</p>
<p>$\gamma ,\beta$的导数公式$p5, p6$已经给出</p>
<p><strong>求$\frac{\partial \ell}{ \partial x_i}$:</strong></p>
<p>由$p1$可$\widehat{x}$导数为：</p>
<script type="math/tex; mode=display">{\frac{\partial \ell}{\partial \widehat{x}_{i}}=\frac{\partial \ell}{\partial y_{i}} \cdot \gamma}  \tag{1}</script><p>$\frac{\partial \ell}{\partial y}$为上一层传下来的导数；</p>
<script type="math/tex; mode=display">\begin{aligned} \frac{\partial \ell}{ \partial x_i} &= \frac{\partial \ell}{ \partial \widehat{x}} (\frac{\partial \widehat{x}}{ \partial x_i} + \frac{\partial \widehat{x}}{ \partial \mu} \frac{\partial \mu}{ \partial x_i}  + \frac{\partial \widehat{x}}{ \partial \sigma^{2}} \frac{\partial \sigma^{2}}{ \partial x_i}) \\ &= \frac{\partial \ell}{ \partial \widehat{x}} \frac{1}{\sqrt{\sigma^2 + \epsilon}}     + (\frac{\partial \ell}{ \partial \widehat{x}} \frac{\partial \widehat{x}}{ \partial \mu}) \frac{\partial \mu}{ \partial x_i} + (\frac{\partial \ell}{ \partial \widehat{x}} \frac{\partial \widehat{x}}{ \partial \sigma^{2}}) \frac{\partial \sigma^{2}}{ \partial x_i} \\ &= \frac{\partial \ell}{ \partial \widehat{x}} \frac{1}{\sqrt{\sigma^2 + \epsilon}}     + \frac{\partial \ell}{ \partial \mu} \frac{1}{m} + \frac{\partial \ell}{ \partial \sigma^{2}}  \frac{\partial \sigma^2}{\partial x_i} \end{aligned} \tag{2}</script><script type="math/tex; mode=display">\begin{align} \frac{\partial \sigma^2}{\partial x_i} &= \frac{2(x_i - \mu)}{m} \end{align} （\mu在这里是个常数）</script><p>带入上式</p>
<script type="math/tex; mode=display">\begin{aligned}  \frac{\partial \ell}{ \partial x_i} = \frac{\partial \ell}{ \partial \widehat{x}} \frac{1}{\sqrt{\sigma^2 + \epsilon}}     + \frac{\partial \ell}{ \partial \mu} \frac{1}{m} + \frac{\partial \ell}{ \partial \sigma^{2}}  \frac{2(x_i - \mu)}{m} \end{aligned} \tag{3}</script><p>接下来要求出$\frac{\partial \ell}{ \partial \mu} , \frac{\partial \ell}{ \partial \sigma^{2}}$:</p>
<p>由上式</p>
<script type="math/tex; mode=display">\begin{aligned} \frac{\partial \ell}{ \partial \mu} &= \frac{\partial \ell}{ \partial \widehat{x}} \frac{\partial \widehat{x}}{\partial \mu } + \frac{\partial \ell}{ \partial \sigma^{2}} \frac{\partial \sigma^{2}}{\partial \mu } \\&= \sum_{j=1}^m \frac{\partial \ell} {\partial \widehat{x}} \cdot \frac{-1}{\sqrt{\sigma^2 + \epsilon}} + \frac{\partial \ell}{ \partial \sigma^{2}} \sum_{m = j}^m \frac{-2(x_i - \mu)}{m} （\mu在这里是个常数） \\&=   \sum_{j=1}^m \frac{\partial \ell} {\partial \widehat{x}} \cdot \frac{-1}{\sqrt{\sigma^2 + \epsilon}} + \frac{\partial \ell}{ \partial \sigma^{2}} \cdot (-2) (\sum_{m = j}^m \frac{x_i}{m} - \sum_{m = j}^m \frac{\mu}{m}) \\ &=  \sum_{j=1}^m \frac{\partial \ell} {\partial \widehat{x}} \cdot\frac{-1}{\sqrt{\sigma^2 + \epsilon}} + \frac{\partial \ell}{ \partial \sigma^{2}} \cdot (-2) (\mu - m \cdot \frac{\mu}{m}) \\&=  \sum_{j=1}^m \frac{\partial \ell} {\partial \widehat{x}} \cdot\frac{-1}{\sqrt{\sigma^2 + \epsilon}}  \end{aligned} \tag{4}</script><p>由$p2$式得出</p>
<script type="math/tex; mode=display">\begin{aligned} \frac{ \partial \ell} {\sigma^2} &=  \sum_{j = m}^m \frac{\partial \ell}{ \partial \widehat{x}} \cdot(-\frac{1}{2}(x_i - \mu)(\sigma^2 + \epsilon))^{-1.5}\\ &=(-\frac{-1}{2})\sum_{j = m}^m (\frac{\partial \ell}{ \partial \widehat{x}} \cdot(x_i - \mu)(\sigma^2 + \epsilon)^{-1.5})  \end{aligned} \tag{5}</script><p>将4式和5式带入3式</p>
<script type="math/tex; mode=display">\begin{aligned}  \frac{\partial \ell}{ \partial x_i} &= \frac{\partial \ell}{ \partial \widehat{x_i}} \frac{1}{\sqrt{\sigma^2 + \epsilon}}  + \sum_{j=1}^m \frac{\partial \ell} {\partial \widehat{x_j}} \cdot\frac{-1}{\sqrt{\sigma^2 + \epsilon}}  \frac{1}{m} + (\frac{-1}{2})\sum_{j = 1}^m (\frac{\partial \ell}{ \partial \widehat{x_j}} (x_i - \mu)(\sigma^2 + \epsilon))^{-1.5} \cdot\frac{2(x_i - \mu)}{m} \\&=  \frac{1}{\sqrt{\sigma^2 + \epsilon}} \frac{\partial \ell}{ \partial \widehat{x_i}}  + ( \frac{-1}{\sqrt{\sigma^2 + \epsilon}}  \frac{1}{m})\sum_{j=1}^m \frac{\partial \ell} {\partial \widehat{x_j}}  + (\frac{-1}{\sqrt{\sigma^2 + \epsilon}} \cdot\frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \frac{1}{m}) \sum_{j = m}^m (\frac{\partial \ell}{ \partial \widehat{x_j}}  \frac{(x_j - \mu)}{\sqrt{\sigma^2 + \epsilon}}) \\&= \frac{1}{m\sqrt{\sigma^2 + \epsilon}} \left [ m\frac{\partial \ell}{ \partial \widehat{x_i}} - \sum_{j = 1}^m\frac{\partial \ell}{ \partial \widehat{x_j}} - \widehat{x_i} \sum_{j=1}^m (\frac{\partial \ell}{\partial \widehat{x_j}}\widehat{x_j})\right ]\end{aligned}  \tag{6}</script><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><script type="math/tex; mode=display">{\frac{\partial \ell}{\partial \widehat{x}_{i}}=\frac{\partial \ell}{\partial y_{i}} \cdot \gamma}  \tag{7}</script><script type="math/tex; mode=display">{\frac{\partial \ell}{\partial \gamma}=\sum_{i=1}^{m} \frac{\partial \ell}{\partial y_{i}} \cdot \widehat{x}_{i}} \tag{8}</script><script type="math/tex; mode=display">{\frac{\partial \ell}{\partial \beta}=\sum_{i=1}^{m} \frac{\partial \ell}{\partial y_{i}}} \tag{9}</script><script type="math/tex; mode=display">\frac{\partial \ell}{ \partial x_i} =\frac{1}{m\sqrt{\sigma^2 + \epsilon}} \left [ m\frac{\partial \ell}{ \partial x_i} - \sum_{j = 1}^m\frac{\partial \ell}{ \partial x_j} - \widehat{x_i} \sum_{j=1}^m (\frac{\partial \ell}{\partial \widehat{x_j}} \widehat{x_j})\right ] \tag{10}</script><p>参考:</p>
<p><a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1502.03167.pdf</a></p>
<p><a href="http://wuli.wiki//online/PChain.html" target="_blank" rel="noopener">http://wuli.wiki//online/PChain.html</a></p>
<p><a href="https://kevinzakka.github.io/2016/09/14/batch_normalization/" target="_blank" rel="noopener">https://kevinzakka.github.io/2016/09/14/batch_normalization/</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/34879333" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34879333</a></p>

  </div>
</article>

    <div class="blog-post-comments">
        <div id="disqus_thread">
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about">About</a></li>
         
          <li><a href="/archives">Writing</a></li>
         
          <li><a href="/tags">tags</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#BatchNormal"><span class="toc-number">1.</span> <span class="toc-text">BatchNormal</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#求导："><span class="toc-number">1.0.1.</span> <span class="toc-text">求导：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#复合函数的链式法则"><span class="toc-number">1.0.2.</span> <span class="toc-text">复合函数的链式法则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#复合函数的偏导-链式法则"><span class="toc-number">1.0.3.</span> <span class="toc-text">复合函数的偏导 链式法则</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#反向传播"><span class="toc-number">1.0.4.</span> <span class="toc-text">反向传播</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#总结"><span class="toc-number">1.0.5.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://d2rivendell.github.io/2019/10/15/BatchNormal/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&text=BatchNormal"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&title=BatchNormal"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&is_video=false&description=BatchNormal"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=BatchNormal&body=Check out this article: http://d2rivendell.github.io/2019/10/15/BatchNormal/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&title=BatchNormal"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&title=BatchNormal"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&title=BatchNormal"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&title=BatchNormal"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://d2rivendell.github.io/2019/10/15/BatchNormal/&name=BatchNormal&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2020 leon
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about">About</a></li>
         
          <li><a href="/archives">Writing</a></li>
         
          <li><a href="/tags">tags</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":75,"height":150},"mobile":{"show":true}});</script></body>
</html>
<!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

<!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-127325381-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

<!-- Disqus Comments -->

    <script type="text/javascript">
        var disqus_shortname = 'leonhwa';

        (function(){
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript';
            dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        }());
    </script>


