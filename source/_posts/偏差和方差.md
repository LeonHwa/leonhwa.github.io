---
title: 偏差和方差
date: 2019-09-22 15:13:15
tags: DL
---



>  $:=$   表示更新参数,

### 高偏差(high bias)

 也称为欠拟合**underfitting**，训练精度不够

无法拟合训练集，那么你要做的就是选择一个新的网络，比如含有更多隐藏层或者隐藏单元的网络，或者花费更多时间来训练网络，或者尝试更先进的优化算法

### 高方差(high variance)

 也称为过拟合**overfitting**， 测试率不高

有两个解决方法，一个是正则化，另一个是准备更多的数据，这是非常可靠的方法，但你可能无法时时刻刻准备

#### L2正则化

$$J (w,b) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) +  \frac{\lambda}{2m} {\left \| w \right \|}_{2}^{2}$$

其中${\left \| w \right \|}_{2}^{2} = \sum_{j = 1}^{n}  w_j^2 = w^T w ​$



新的梯度下降（gradient descent）公式为

$$w :=\left(1-\frac{\eta \lambda}{n}\right) w-\frac{\eta}{m} \sum_{x} \frac{\partial C_{x}}{\partial w}​$$

$$b := b-\frac{\eta}{m} \sum_{x} \frac{\partial C_{x}}{\partial b}$$

为什么L2正则化可以防止过拟合呢？

直观上理解就是如果正则化$\lambda​$设置得足够大，权重矩阵𝑊被设置为接近于0的值，直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近高偏差状态。

$\lambda$ 增大， $w$ 减小， $z​$减小，在一定程度会使得激活函数（比如tanh）处于线性部分


![tanh](/images/tanh.png)

