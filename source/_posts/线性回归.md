---
title: 线性回归
date: 2019-10-02 23:08:16
tags: ML
---

#### 基本概念
**监督学习：** 根据样本特征和对应结果，对新的样本作出预测，如预测房价（回归问题）和是否是肿瘤（分类问题）

假设方案函数为
$
h(\theta) = \theta_0 + \theta_1 x\quad(1)
$

代价函数(cost function)

$
J(\theta_0,\theta_1) =  \frac{1}{2m}\sum_{i=1}^{m}(h(\theta^{(i)} - y^{(i)})^2\quad(2)
$

多维特征

$X$是个 m x n的训练集
$X_j^{(i)}$ 表示第i个训练集的第j个特征

若假设函数h支持多变量(参数有n+1个，训练集有n个)
$
h_{\theta}(x) = \theta_0 + \theta_1 x_1 + ...+\theta_0 x_n\quad(3)
$
在此公式上引入$x_0= 1$,原始的参数和训练集都变成n+1
变成如下
$
h_{\theta}(x) = \theta_0 x_0 + \theta_1 x_1 + ...+\theta_0 x_n\quad(4)
$
这样做的原因是求导的时候方便
#### 梯度下降
如$\theta_n$为二维

$
\begin{aligned} & \text { repeat until convergence }\{ \\ \theta_{j} :=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J\left(\theta_{0}, \theta_{1}\right) &(\text { for } j=0 \text { and } j=1)  \\
\} \end{aligned} 
$

$\alpha​$是学习率

$
\begin{array}{l}{\operatorname{temp} 0 :=\theta_{0}-\alpha \frac{\partial}{\partial \theta_{0}} J\left(\theta_{0}, \theta_{1}\right)} \\ {\operatorname{temp} 1 :=\theta_{1}-\alpha \frac{\partial}{\partial \theta_{1}} J\left(\theta_{0}, \theta_{1}\right)} \\ {\theta_{0} :=\operatorname{temp} \theta} \\ {\theta_{1} :=\operatorname{tem} \mathrm{p} 1}\end{array}
​$
偏导数表示沿着各个纬度的方式去找到极小值，`$\alpha$`中途不需要改变，假设最开始时在最陡的坡上，导数假如是大于0，此时导数比较大（陡）， 通过梯度下降算法，`$\theta$`会减小，慢慢靠近坡底，同时该点的导数也会减小，通过一步步计算，导数慢慢变小，`$\theta$`减小的幅度越来越小，`$\alpha$`的值不能太小（下降到最低点需要太多步），或太大（找不到最低点）。

对线性回归问题的代价函数进行偏导计算
$
\begin{array}{l}{h_{\theta}(x)=\theta_{0}+\theta_{1} x} \\ {J(\theta)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}}\end{array}
$




$
\frac{\partial}{\partial \theta_{j}} J\left(\theta_{0}, \theta_{1}\right)=\frac{\partial}{\partial \theta_{j}} \frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
$

$
j = 0:
\frac{\partial}{\partial \theta_{0}} J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)
$


$
j = 1:
\frac{\partial}{\partial \theta_{1}} J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{m} \sum_{i=1}^{m}\left(\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x^{(i)}\right)
$

对于多维变量线性回归
代价函数变为

$
J\left(\theta_{0}, \theta_{1} \ldots \theta_{n}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}
$
其中$h_{\theta}(x)=\theta^{T} X=\theta_{0}+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}$`,为特征集引入`$x_0 = 1$`(方便一起计算偏置b)后变成`$h_{\theta}(x)=\theta^{T} X=\theta_{0}x_0+\theta_{1} x_{1}+\theta_{2} x_{2}+\ldots+\theta_{n} x_{n}$
梯度下降公式为

$
\theta_{j} :=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}
$

#### python代码实现

```python
df = pd.read_csv('ex1data1.txt', names=['population', 'profit'])

def get_X(df):#读取特征

    ones = pd.DataFrame({'ones': np.ones(len(df))})#ones是m行1列的dataframe
    data = pd.concat([ones, df], axis=1)  # 合并数据，根据列合并,相当于令每个特征集的x_0 = 1
    return data.iloc[:, :-1].as_matrix()  # 这个操作返回 ndarray,不是矩阵
#代价函数
def lr_cost(theta, X, y):
#     """
#     X: R(m*n), m 样本数, n 特征数
#     y: R(m)
#     theta : R(n), 线性回归的参数
#     """
    m = X.shape[0]#m为样本数

    inner = X @ theta - y  # R(m*1)，X @ theta等价于X.dot(theta)

    # 1*m @ m*1 = 1*1 in matrix multiplication
    # but you know numpy didn't do transpose in 1d array, so here is just a
    # vector inner product to itselves
    square_sum = inner.T @ inner
    cost = square_sum / (2 * m)

    return cost
    
#批量操作
def gradient(theta, X, y):
    m = X.shape[0]

    inner = X.T @ (X @ theta - y)  # (m,n).T @ (m, 1) -> (n, 1)，X @ theta等价于X.dot(theta)

    return inner / m
    
#批量梯度下降函数(一次性对所有样本进行梯度下降算法)
def batch_gradient_decent(theta, X, y, epoch, alpha=0.01):
#   拟合线性回归，返回参数和代价
#     epoch: 批处理的轮数
#     """
    cost_data = [lr_cost(theta, X, y)]
    _theta = theta.copy()  # 拷贝一份，不和原来的theta混淆

    for _ in range(epoch):
        _theta = _theta - alpha * gradient(_theta, X, y)
        cost_data.append(lr_cost(_theta, X, y))

    return _theta, cost_data
    
#另外一种 梯度下降方法（比较啰嗦，但是是按照公式一步步完成）
def gradientDescent(X, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape))
    parameters = int(theta.ravel().shape[1])
    cost = np.zeros(iters)
    
    for i in range(iters):
        error = (X * theta.T) - y
        
        for j in range(parameters):
            term = np.multiply(error, X[:,j])
            temp[0,j] = theta[0,j] - ((alpha / len(X)) * np.sum(term))
            
        theta = temp
        cost[i] = computeCost(X, y, theta)
        
    return theta, cost
```

### normal equation  正规方程
通过令代价函数的导数的为0求出极值点

$
\frac{\partial}{\partial \theta_{j}} J\left(\theta_{j}\right)=0
$

得出$\theta=\left(X^{T} X\right)^{-1} X^{T} y$ （其中$X$包含了$x_0 = 1$）

```python
# 正规方程
def normalEqn(X, y):
    theta = np.linalg.inv(X.T@X)@X.T@y#X.T@X等价于X.T.dot(X)
    return theta
```

