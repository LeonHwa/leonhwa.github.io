---
title: 反向传播公式总结
date: 2019-10-25 23:29:38
tags: DL
---



### 相关公式

已知

 $$z= w x+ b \quad(1)$$

 $$a = g(z) =\frac{1}{1+e^{-z}} \quad(2)$$

 $$g^{\prime}(z) = g(z)(1-g(z)) \quad(3)$$

$ L = -y \ln a - (1-y) \ln (1-a) \quad(4)$

$$J(w,b) = \frac{1}{m} \sum_{i=1}^{m}  L (\hat{y}^{(i)},  y^{(i)}) = \frac{1}{m} \sum_{i = 1}^{m}\left (-y^{(i)} log\hat{y}^{(i)} - (1 - y^{(i)} ) log(1- \hat{y}^{(i)}) \right) \quad(5)$$



四个基本方程

$\delta^{L}=\nabla_{a} C \odot \sigma^{\prime}\left(z^{L}\right)  \quad(\mathrm{BP} 1)$

$\delta^{l}=\left(\left(w^{l+1}\right)^{T} \delta^{l+1}\right) \odot \sigma^{\prime}\left(z^{l}\right) \quad(\mathrm{BP} 2)$

$\frac{\partial C}{\partial b_{j}^{L}}=\delta_{j}^{l} \quad(\mathrm{BP} 3)$

$\frac{\partial C}{\partial w_{j k}^{l}}=a_{k}^{l-1} \delta_{j}^{l} \quad(\mathrm{BP} 4)$



由BP1和BP2可知

$\frac{\partial L(w,b)}{\partial a}=\left(w^{l+1}\right)^{T} \delta^{l+1} \quad(6)$

对于最后一个输出的A的导数$\frac{\partial L(w,b)}{\partial a}$为：

$ \begin{aligned}  \frac{\partial L(w,b)}{\partial a} &=  -\frac{y}{a} + \frac{1-y}{1-a}   \\ & = \frac{a- y}{a(1-a)} \end{aligned} \quad(7) $



为了能更灵活地组合线性函数和激活函数，线性函数，激活函数地向前传播和向后传播都独立地写

在这里计算dz 都是用BP1公式（http://neuralnetworksanddeeplearning.com/ 中是最后一层用BP1算出dz， 再利用BP2算出前面的dz，再一直反向推导出dw,db)

![](/images/DNN反向传播.png)

## 代码实现

####借助dA计算进行反向传播

最后一个输出的dA可以使用公式7计算出来

部分公式如下

```python
    
  def sigmoid_backward(dA, cache):
    """
    dz = (delta C / delta A) * activation_derivative
    :param dA: (delta C / delta A)
    :param cache: z
    :return:
    """
    z = cache
    dz = dA * sigmoid_derivate(z)
    return dz
  def linear_backward(self, dz, cache):
        """
        只包含线性函数的反向传播计算
        :param dz: delta值 (n_2, m)
        :param cache: 包含 A_prev(n_1, m), w(n_2, n_1), b(n_2, 1)
        :return:  dA_prev, dw, db
        """
        A_prev, w, b = cache
        m = A_prev.shape[1]
        dw = (dz @ A_prev.T)/m  # BP4 
        db = np.sum(dz, axis=1, keepdims=True)/m
        dA_prev = w.T @ dz  # 结合BP1和BP2
        return dA_prev, dw, db
      
dAL = -np.divide(Y, AL) + np.divide(1 - Y, 1 - AL) 
dz = sigmoid_backward(dA, cache[1]) #cache = ((a,w,b), z)
dA_prev, dw, db = linear_backward(self, dz, cache[0]) #cache = ((a,w,b), z)
...
...

      
```



#### 借助dz计算进行反向传播

最后一个输出的dz可以使用BP1 公式7计算出来 （和http://neuralnetworksanddeeplearning.com/ 一致）

由公式7得知$  \frac{\partial L(w,b)}{\partial a} =  -\frac{y}{a} + \frac{1-y}{1-a}   = \frac{a- y}{a(1-a)} $

对于激活函数为sigmoid函数时，由公式3可知$g^{\prime}(z) = g(z)(1-g(z))$

 $\frac{\partial L(w,b)}{\partial z} = \frac{\partial L(w,b)}{\partial a}\frac{\partial a}{\partial z} = \frac{a- y}{a(1-a)} * a(1-a) = a - y$



```python
 def backward_propagation(self, X, Y):
        """
        只有一个隐藏层的简单神经网络的反向传播
        :param X: (n, m)
        :param Y:
        :return:
        """
        m = X.shape[1]
        w2 = self.parameters["w2"]
        A2, cache = self.forward_propagation(X)
        A1 = cache["A1"]  # (m_1 x m_0)
        z1 = cache["z1"]
        dz2 = (A2 - Y)  # (m_2 x m_0) 参考《损失函数和代价函数》公式7  公式11
        dw2 = (dz2 @ A1.T)/m  # (m_2 x m_1)
        db2 = np.sum(dz2, axis=1, keepdims=True)/m

        dz1 = np.multiply((w2.T @ dz2), tanh_derivate(z1))
        dw1 = (dz1 @ X.T)/m
        db1 = np.sum(dz1, axis=1, keepdims=True)/m
        back_pro = {"dw1": dw1,
                    "db1": db1,
                    "dw2": dw2,
                    "db2": db2,
                    }
        return back_pro, cache
```

