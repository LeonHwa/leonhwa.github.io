---
title: 逻辑回归
date: 2019-10-02 23:13:38
tags: DL
---

逻辑回归属于分类问题

逻辑回归是在线性回归的基础上加一个激活函数，使得变得非线性
$$
h_{\theta}(x)=g\left(\theta^{T} x\right)
$$

$$
g(z)=\frac{1}{1+e^{-z}}
$$
### cost function

$$
J(\theta)=\frac{1}{m} \sum_{i=1}^{m} \operatorname{cost}\left(h_{\theta}\left(x^{(i)}\right), y^{(i)}\right)
$$

其中$cost(h_{\theta},y)$如下
$$
\operatorname{cost}\left(h_{\theta}(x), y\right)=\left\{\begin{aligned}-\log \left(h_{\theta}(x)\right) & \text { if } y=1 \\-\log \left(1-h_{\theta}(x)\right) & \text { if } y=0 \end{aligned}\right.
$$
简化为：

$$
\operatorname{cost}\left(h_{\theta}(x), y\right)=-y \times \log \left(h_{\theta}(x)\right)-(1-y) \times \log \left(1-h_{\theta}(x)\right)
$$

带入上式得

$$
J(\theta)=\frac{1}{m} \sum_{i=1}^{m}\left[-y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)-\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]
$$

##### python代码实现：

```python
#sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

cost

```python
#cost function 
def cost(theta, X, y):
    ''' cost fn is -l(theta) for you to minimize'''
    return np.mean(-y * np.log(sigmoid(X @ theta)) - (1 - y) * np.log(1 - sigmoid(X @ theta)))

```

#### gradient descent(梯度下降)

逻辑回归的损失函数使用的是cross-entropy，区别于线性回归的平方差损失函数， 

 $z= w x+ b $

$a = g(z) =\frac{1}{1+e^{-z}}$

$$g^{\prime}(z) = g(z)(1-g(z)) ​$$

$ L = -y \ln a - (1-y) \ln (1-a) $

对损失函数w求偏导

$$ \begin{aligned}  \frac{\partial L(w,b)}{\partial w}  &=  -\frac{y}{a}a' + \frac{1-y}{1-a} a'  \\ & = \frac{a- y}{a(1-a)} a'\end{aligned} $$

又因为

$$\begin{aligned}  \frac{\partial a}{\partial w} &= \frac{\partial a}{\partial z}  \frac{\partial z}{\partial w}   \\ &= a(1-a)  \end{aligned} ​$$

带入上式得

$ \frac{\partial L(w,b)}{\partial w}  = a - y  $

得出代价函数w偏导数为

$$\frac{\partial J(w, b)}{\partial w} = \frac{1}{m} \sum_{i=1}^m (a^i - y^i)x^i ​$$

同理得代价函数b偏导数为

$\frac{\partial J(w, b)}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^i - y^i)$ （在逻辑回归中一般不使用，在逻辑回归中，给样本特征插入1 可以巧妙地一起计算w和b）


$$
\begin{aligned} \frac{\partial \boldsymbol{J}(\theta)}{\partial \boldsymbol{\theta}_{j}}=\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)} &\\

\theta_{j} :=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}
\end{aligned}
$$


```python
def gradient(theta, X, y):
    '''just 1 batch gradient'''
    return (1 / len(X)) * X.T @ (sigmoid(X @ theta) - y)

```


### 防止过拟合---正则化

##### regularized coast （正则化代价函数）

$$
J(\theta)=\frac{1}{m} \sum_{i=1}^{m}\left[-y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)-\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}
$$



```python
#正则化代价函数
def regularized_cost(theta, X, y, l=1):
#     '''you don't penalize theta_0'''
    theta_j1_to_n = theta[1:]
    regularized_term = (l / (2 * len(X))) * np.power(theta_j1_to_n, 2).sum()

    return cost(theta, X, y) + regularized_term

```

##### regularized gradient (正则化梯度)

$$
\frac{\partial \boldsymbol{J}(\theta)}{\partial \boldsymbol{\theta}_{j}}=\left(\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)\right)+\frac{\lambda}{m} \theta_{j} \text { for } \mathbf{j} \geq 1
$$



```python
def regularized_gradient(theta, X, y, l=1):
#     '''still, leave theta_0 alone'''
    theta_j1_to_n = theta[1:]
    regularized_theta = (l / len(X)) * theta_j1_to_n

    # by doing this, no offset is on theta_0
    regularized_term = np.concatenate([np.array([0]), regularized_theta])

    return gradient(theta, X, y) + regularized_term

```


### 
