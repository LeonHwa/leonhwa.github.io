---
title: 损失函数和代价函数
date: 2019-09-21 23:32:57
tags: DL
---

> $\hat{y}$: 训练集的预测值 
>
> $y$: 训练集的实际值 
>
> 上标 (𝑖)来指明 数据表示 𝑥或者 𝑦或者 𝑧或者其他数据的第 𝑖个训样本，如$x^{(i)},z^{(i)} $

### 损失函数（误差函数）

用来衡量算法运行情况，可以表示为$\text { Loss function: } L(\hat{y}, y)$

**平方差损失函数(quadratic)**：
$L=\frac{(y-a)^{2}}{2}$

一般用于线性回归



**cross-entropy**:  

$ L = -y \ln a - (1-y) \ln (1-a)$

用于逻辑回归，以及神经网络中（废话）



### 代价函数

损失函数只适用于的单个训练样本，而代价函数是参数的总代价，衡量算法在全部训练样本上的表现

对于cross-entropy的代价函数可表示为

$$J(w,b) = \frac{1}{m} \sum_{i=1}^{m}  L (\hat{y}^{(i)},  y^{(i)}) = \frac{1}{m} \sum_{i = 1}^{m}\left (-y^{(i)} log\hat{y}^{(i)} - (1 - y^{(i)} ) log(1- \hat{y}^{(i)}) \right)$$



### 疑惑

为什么逻辑回归的损失函数的的表达式是这样呢？ 有对数，又有1-y，又带有负号

对于$S$型激活函数 $\sigma(z) = \sigma(w^T x + b) = \frac{1}{1+e{-z}}$，约定$\hat{y} = p(y = 1 |x)$ ，即$\hat{y} $ 表示给定训练样本$x$条件下$y$于1的概率 

因此，如果$\hat{y}$ 代表$y = 1$的概率，那么$1 - \hat{y}$就是$y = 0$的概率。

$\begin{array}{ll}{\text { If } \quad y=1 :} & {p(y | x)=\hat{y}} \\ {\text { If } \quad y=0 :} & {p(y | x)=1-\hat{y}}\end{array}$

上面两式合并可得出$p(y|x) $完整定义

$$p(y|x) = \hat{y}^y (1 - \hat{y})^{(1-y)}$$

由于$log$ 函数是严格单调递增的函数，最大化$log(p(y|x))$等价于最大化$p(y|x) $并且地计算$p(y|x) $ 的$log$对数，即计算$log( \hat{y}^y (1 - \hat{y})^{(1-y)})$(其实就是将$p(y|x) $代入)，通过对数函数化简为：

$$ylog(\hat{y}^y)  +(1-y) log(1 - \hat{y})$$



逻辑回归损失函数前面有负号的原因是：当训练学习算法时需要算法输出值的概率是最大的（以最大的概率预测这个值），然而在逻辑回归中我们需要最小化损失函数，因此最小化损失函数与最大化条件概率的对数𝑙𝑜𝑔(𝑝(𝑦|𝑥)) 关联起来了，因此这就是单个训练样本的损失函数表达式。

$$-ylog(\hat{y}^y)  -(1-y) log(1 - \hat{y})$$



最大似然估计？