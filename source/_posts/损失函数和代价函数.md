---
title: 损失函数和代价函数
date: 2019-09-21 23:32:57
tags: DL
---

> $\hat{y}$: 训练集的预测值 
>
> $y$: 训练集的实际值 
>
> 上标 (𝑖)来指明 数据表示 𝑥或者 𝑦或者 𝑧或者其他数据的第 𝑖个训样本，如$x^{(i)},z^{(i)} $

### 损失函数（误差函数）

用来衡量算法运行情况，可以表示为$\text { Loss function: } L(\hat{y}, y)​$

**平方差损失函数(quadratic)**：
$L=\frac{(y-a)^{2}}{2}​$

一般用于线性回归



**cross-entropy**:  

$ L = -y \ln a - (1-y) \ln (1-a)​$

用于逻辑回归，以及神经网络中（废话）



### 代价函数

损失函数只适用于的单个训练样本，而代价函数是参数的总代价，衡量算法在全部训练样本上的表现

对于cross-entropy的代价函数可表示为

$$J(w,b) = \frac{1}{m} \sum_{i=1}^{m}  L (\hat{y}^{(i)},  y^{(i)}) = \frac{1}{m} \sum_{i = 1}^{m}\left (-y^{(i)} log\hat{y}^{(i)} - (1 - y^{(i)} ) log(1- \hat{y}^{(i)}) \right)​$$

#### 代价函数偏导数

已知

$z= w x+ b \tag{1}​$

$a = g(z) =\frac{1}{1+e^{-z}} \tag{2}​$

$$g^{\prime}(z) = g(z)(1-g(z)) \tag{3}​$$

$ L = -y \ln a - (1-y) \ln (1-a) \tag{4}​$

对损失函数w求偏导

$$ \begin{aligned}  \frac{\partial L(w,b)}{\partial w}  &=  -\frac{y}{a}a' + \frac{1-y}{1-a} a'  \\ & = \frac{a- y}{a(1-a)} a'\end{aligned} \tag{5}​$$

又因为

$$\begin{aligned}  \frac{\partial a}{\partial w} &= \frac{\partial a}{\partial z}  \frac{\partial z}{\partial w}   \\ &= a(1-a)  \end{aligned} \tag{6}​$$

带入上式得

$ \frac{\partial L(w,b)}{\partial w}  = a - y \tag{7} ​$

得出代价函数w偏导数为

$$\frac{\partial J(w, b)}{\partial w} = \frac{1}{m} \sum_{i=1}^m (a^i - y^i)x^i \tag{8}​$$



同理得代价函数b偏导数为

$$\frac{\partial J(w, b)}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^i - y^i) \tag{9}$$

### 疑惑

为什么逻辑回归的损失函数的的表达式是这样呢？ 有对数，又有1-y，又带有负号

对于$S​$型激活函数 $\sigma(z) = \sigma(w^T x + b) = \frac{1}{1+e{-z}}​$，约定$\hat{y} = p(y = 1 |x)​$ ，即$\hat{y} ​$ 表示给定训练样本$x​$条件下$y​$于1的概率 

因此，如果$\hat{y}$ 代表$y = 1$的概率，那么$1 - \hat{y}$就是$y = 0$的概率。

$\begin{array}{ll}{\text { If } \quad y=1 :} & {p(y | x)=\hat{y}} \\ {\text { If } \quad y=0 :} & {p(y | x)=1-\hat{y}}\end{array}$

上面两式合并可得出$p(y|x) $完整定义

$$p(y|x) = \hat{y}^y (1 - \hat{y})^{(1-y)}$$

由于$log$ 函数是严格单调递增的函数，最大化$log(p(y|x))$等价于最大化$p(y|x) $并且地计算$p(y|x) $ 的$log$对数，即计算$log( \hat{y}^y (1 - \hat{y})^{(1-y)})$(其实就是将$p(y|x) $代入)，通过对数函数化简为：

$$ylog(\hat{y}^y)  +(1-y) log(1 - \hat{y})$$



逻辑回归损失函数前面有负号的原因是：当训练学习算法时需要算法输出值的概率是最大的（以最大的概率预测这个值），然而在逻辑回归中我们需要最小化损失函数，因此最小化损失函数与最大化条件概率的对数𝑙𝑜𝑔(𝑝(𝑦|𝑥)) 关联起来了，因此这就是单个训练样本的损失函数表达式。

$$-ylog(\hat{y}^y)  -(1-y) log(1 - \hat{y})$$



最大似然估计？