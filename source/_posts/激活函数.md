---
title: 激活函数
date: 2019-09-26 23:58:29
tags: DL
---

![activation](/images/activation.png)



### **tanh**和**sigmoid**的区别

效果总是优于**sigmoid**函数。因为函数值域在-1和+1的激活函数，其均值是更接近零均值的。在训练一个算法模型时，如果使用**tanh**函数代替**sigmoid**函数中心化数据，使得数据的平均值更接近0而不是0.5



###  **tanh**和**sigmoid**的共同缺点

函数两者共同的缺点是，在𝑧特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。



### **ReLu**和**Leaky ReLu**的优点是：

- 在$z$的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个**if-else**语句，而**sigmoid**函数需要进行浮点四则运算，在实践中，使用**ReLu**激活函数神经网络通常会比使用**sigmoid**或者**tanh**激活函数学习的更快。
- **sigmoid**和**tanh**函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而**Relu**和**Leaky ReLu**函数大于0部分都为常数，不会产生梯度弥散现象。(同时应该注意到的是，**Relu**进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而**Leaky ReLu**不会有这问题) 

### 使用总结

**sigmoid**激活函数：除了输出层是一个二分类问题基本不会用它。

**tanh**激活函数：**tanh**是非常优秀的，几乎适合所有场合。

**ReLu**激活函数：最常用的默认函数，，如果不确定用哪个激活函数，就使用**ReLu**或者**Leaky ReLu**。



### 导数

#### sigmoid

$$g(z) = \frac{1}{1+e^{-z}}​$$

$$g^{\prime}(z) = g(z)(1-g(z))$$

#### tanh

$$g(z)=\tanh (z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$$

$$g^{\prime}(z) = 1 - (tanh(z))^2$$



#### ReLU(Rectified Linear Unit)

$$g(z) =max(0,z)$$

$$g(z)^{\prime}=\left\{\begin{array}{ll}{0} & {\text { if } \quad z<0} \\ {1} & {\text { if } \quad z>0} \\ {\text {undefined}} & {\text { if } \quad z=0}\end{array}\right.$$



#### Leaky ReLU(Leaky Linear Unit)

$$g(z) =max(0.01,z)​$$

$$g(z)^{\prime}=\left\{\begin{array}{ll}{0.01z} & {\text { if } \quad z<0} \\ {1} & {\text { if } \quad z>0} \\ {\text {undefined}} & {\text { if } \quad z=0}\end{array}\right.$$

